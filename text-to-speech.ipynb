{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**this notebook was trained on kaggle, for reproducing the results or for running, fork my notebook ðŸ‘‰ [tom/text_to_speech](https://www.kaggle.com/code/blessontomjoseph/text-to-speech/edit)**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install speechbrain\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install --upgrade accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import SpeechT5Processor\n","from transformers import SpeechT5ForTextToSpeech\n","from transformers import SpeechT5HifiGan\n","from datasets import load_dataset,Audio\n","import soundfile as sf\n","\n","import os\n","import torch\n","import torchaudio\n","import datasets\n","import pandas as pd\n","from torch import manual_seed\n","from datasets import Dataset\n","\n","from speechbrain.pretrained import EncoderClassifier\n","import warnings\n","warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["manual_seed(32)\n","class Config:\n","    device='cuda'if torch.cuda.is_available() else 'cpu'\n","    sampling_rate=16000"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Pretrained import"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n","model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n","model=model.to(Config.device)\n","vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\") \n","embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n","speaker_embeddings = torch.tensor(embeddings_dataset[0][\"xvector\"]).unsqueeze(0) "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Formatting data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["emo_abv={'fru': 'Frustration',\n","'exc': 'Excitement',\n","'neu': 'Neutral',\n","'ang': 'Anger',\n","'sad': 'Sadness',\n","'hap': 'Happiness',\n","'sur': 'Surprise',\n","'fea': 'Fear',\n","'oth': 'Other',\n","'dis': 'Disgust'}\n","\n","path=\"/kaggle/input/iemocap-transcriptions-english-french/iemocapTrans.csv\"\n","df=pd.read_csv(path)\n","df.drop(['_id','translated'],axis=1,inplace=True)\n","df['emotion']=df['emotion'].apply(lambda x: emo_abv[x])\n","df.columns=['activation', 'dominance', 'emotion', 'end_time', 'start_time', 'audiofile_name','text', 'valence']\n","\n","audio_path=\"/kaggle/input/iemocap-transcriptions-english-french/Iemocap_audio/iemocap_audio/IEMOCAP_wav\"\n","df['audio']=df['audiofile_name'].apply(lambda x: torchaudio.load(os.path.join(audio_path,x+\".wav\"))[0])\n","df['text']=df['text']+\" [emotion] \"+df['emotion']\n","\n","df_=df[['text','audio']]\n","dataset = Dataset.from_dict({'text':df_['text'],'target':df_['audio'].to_list()})"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Tokenizing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n","speaker_model = EncoderClassifier.from_hparams(\n","    source=spk_model_name, \n","    run_opts={\"device\": Config.device}, \n","    savedir=os.path.join(\"/tmp\", spk_model_name)\n",")\n","\n","def create_speaker_embedding(waveform):\n","    with torch.no_grad():\n","        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n","        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n","        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n","    return speaker_embeddings\n","\n","def prepare_dataset(single_data):\n","    example = processor(\n","        text=single_data[\"text\"],\n","        audio_target=single_data['target'], \n","        sampling_rate=Config.sampling_rate,\n","        return_attention_mask=False,\n","    )\n","    example[\"labels\"] = example[\"labels\"][0]\n","    example[\"speaker_embeddings\"] = create_speaker_embedding(single_data['target'])\n","    return example\n","\n","dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def is_not_too_long(input_ids):\n","    input_length = len(input_ids)\n","    return input_length < 200\n","\n","dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n","dataset = dataset.train_test_split(test_size=0.1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Batching"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class TTSDataCollatorWithPadding:\n","    processor: Any\n","        \n","    def __call__(self, features):\n","        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n","        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n","        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n","\n","        batch = processor.pad(\n","            input_ids=input_ids,\n","            labels=label_features,\n","            return_tensors=\"pt\")        \n","\n","        batch[\"labels\"] = batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)\n","        del batch[\"decoder_attention_mask\"]\n","\n","        if model.config.reduction_factor > 1:\n","            target_lengths = torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n","            target_lengths = target_lengths.new([length - length % model.config.reduction_factor for length in target_lengths])\n","            max_length = max(target_lengths)\n","            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n","        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n","        \n","        return batch\n","    \n","data_collator = TTSDataCollatorWithPadding(processor=processor)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","from transformers import Seq2SeqTrainer\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"/kaggle/working/out_dir\", \n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=2,\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    greater_is_better=False,\n","    label_names=[\"labels\"],\n","    num_train_epochs=1.0)\n","\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    data_collator=data_collator,\n","    tokenizer=processor.tokenizer)\n","\n","trainer.train()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Infer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.display import Audio\n","\n","example = dataset[\"test\"][304]\n","speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n","torch.save(speaker_embeddings,'speaker_embeddings.pt')\n","\n","def gen(prompt,emo,model,speaker_embeddings):\n","    text=prompt+\" [emotion] \"+emo\n","    inputs = processor(text=text, return_tensors=\"pt\")\n","    spectrogram = model.generate_speech(inputs[\"input_ids\"].to('cpu'), speaker_embeddings.to('cpu'))\n","    with torch.no_grad():\n","            speech=vocoder(spectrogram)\n","    return Audio(speech.cpu().numpy(), rate=16000)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["audio=gen('where are you going you think','Anger',trainer.model.to('cpu'),torch.load('speaker_embeddings.pt'))\n","audio"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
